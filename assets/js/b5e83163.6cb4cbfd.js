"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[958],{4706:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-03-isaac-brain/week-10-rl","title":"Week 10: Reinforcement Learning","description":"Training Robots to Learn Like Humans","source":"@site/docs/module-03-isaac-brain/week-10-rl.md","sourceDirName":"module-03-isaac-brain","slug":"/module-03-isaac-brain/week-10-rl","permalink":"/docs/module-03-isaac-brain/week-10-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac-brain/week-10-rl.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Week 10: Reinforcement Learning","sidebar_position":3},"sidebar":"modulesSidebar","previous":{"title":"Perception & Control","permalink":"/docs/module-03-isaac-brain/perception"},"next":{"title":"Week 11: Humanoid Development","permalink":"/docs/module-04-vla/week-11-humanoid"}}');var s=i(4848),a=i(8453);const t={title:"Week 10: Reinforcement Learning",sidebar_position:3},o="Week 10: Reinforcement Learning for Robot Control",l={},c=[{value:"Training Robots to Learn Like Humans",id:"training-robots-to-learn-like-humans",level:2},{value:"RL Fundamentals",id:"rl-fundamentals",level:2},{value:"The RL Loop",id:"the-rl-loop",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:2},{value:"Policy Gradient Methods",id:"policy-gradient-methods",level:3},{value:"Off-Policy Methods",id:"off-policy-methods",level:3},{value:"Robotic RL Environments",id:"robotic-rl-environments",level:2},{value:"OpenAI Gym Interface",id:"openai-gym-interface",level:3},{value:"Isaac Gym (Preview) / Isaac Lab",id:"isaac-gym-preview--isaac-lab",level:3},{value:"Reward Engineering",id:"reward-engineering",level:2},{value:"The Art of Shaping Rewards",id:"the-art-of-shaping-rewards",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"The Reality Gap",id:"the-reality-gap",level:3},{value:"Domain Randomization (DR)",id:"domain-randomization-dr",level:3},{value:"\ud83c\udfaf Weekly Project: RL-Based Locomotion",id:"-weekly-project-rl-based-locomotion",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-10-reinforcement-learning-for-robot-control",children:"Week 10: Reinforcement Learning for Robot Control"})}),"\n",(0,s.jsx)(n.h2,{id:"training-robots-to-learn-like-humans",children:"Training Robots to Learn Like Humans"}),"\n",(0,s.jsxs)(n.p,{children:["This week introduces ",(0,s.jsx)(n.strong,{children:"Reinforcement Learning (RL)"}),", the paradigm shift enabling robots to learn complex skills through trial and error. Instead of hard-coding behaviors, you'll create environments where agents learn to walk, grasp, and navigate by maximizing rewards. We'll leverage Isaac Gym for massively parallel training, compressing years of robotic experience into minutes of simulation time."]}),"\n",(0,s.jsx)(n.h2,{id:"rl-fundamentals",children:"RL Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"the-rl-loop",children:"The RL Loop"}),"\n",(0,s.jsxs)(n.p,{children:["The interaction between an ",(0,s.jsx)(n.strong,{children:"Agent"})," and an ",(0,s.jsx)(n.strong,{children:"Environment"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["State (",(0,s.jsx)(n.code,{children:"$s_t$"}),")"]}),": The agent observes the current situation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Action (",(0,s.jsx)(n.code,{children:"$a_t$"}),")"]}),": The agent takes an action based on its policy ",(0,s.jsx)(n.code,{children:"$\\pi$"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Reward (",(0,s.jsx)(n.code,{children:"$r_t$"}),")"]}),": The environment provides feedback on the action's quality."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Next State (",(0,s.jsx)(n.code,{children:"$s_{t+1}$"}),")"]}),": The environment transitions to a new state."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Maximize cumulative reward ",(0,s.jsx)(n.code,{children:"$R = \\sum \\gamma^t r_t$"})," over time."]}),"\n",(0,s.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Policy (",(0,s.jsx)(n.code,{children:"$\\pi$"}),")"]}),": The brain. Maps states to actions (",(0,s.jsx)(n.code,{children:"$a = \\pi(s)$"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Value Function (",(0,s.jsx)(n.code,{children:"$V(s)$"}),")"]}),": Prediction of future rewards from state ",(0,s.jsx)(n.code,{children:"$s$"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": Representation of how the environment works (optional)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deep-reinforcement-learning-algorithms",children:"Deep Reinforcement Learning Algorithms"}),"\n",(0,s.jsx)(n.h3,{id:"policy-gradient-methods",children:"Policy Gradient Methods"}),"\n",(0,s.jsx)(n.p,{children:"Optimize the policy directly to maximize rewards."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"PPO (Proximal Policy Optimization)"}),"\nThe industry standard for continuous control in robotics."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stability"}),": Prevents large, destructive policy updates."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sample Efficiency"}),": Reuses data for multiple training steps."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# PPO Update (Concept)\ndef ppo_update(policy, value_net, batch):\n    # Calculate advantages\n    advantages = calculate_gae(batch['rewards'], batch['values'])\n    \n    # Policy loss (clipped surrogate objective)\n    ratio = torch.exp(new_log_probs - old_log_probs)\n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1-eps, 1+eps) * advantages\n    policy_loss = -torch.min(surr1, surr2).mean()\n    \n    # Value loss\n    value_loss = mse_loss(value_net(states), returns)\n    \n    # Backpropagate\n    total_loss = policy_loss + 0.5 * value_loss\n    optimizer.step(total_loss)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"off-policy-methods",children:"Off-Policy Methods"}),"\n",(0,s.jsx)(n.p,{children:"Re-use past experiences stored in a replay buffer."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SAC (Soft Actor-Critic)"}),": Maximizes reward + entropy (randomness) for robust exploration."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DQN (Deep Q-Network)"}),": For discrete action spaces (less common in manipulation)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"robotic-rl-environments",children:"Robotic RL Environments"}),"\n",(0,s.jsx)(n.h3,{id:"openai-gym-interface",children:"OpenAI Gym Interface"}),"\n",(0,s.jsx)(n.p,{children:"Standard API for RL environments."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import gym\n\nclass RobotEnv(gym.Env):\n    def __init__(self):\n        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(7,))\n        self.observation_space = gym.spaces.Box(low=-inf, high=inf, shape=(20,))\n        \n    def reset(self):\n        # Reset robot to initial state\n        return initial_state\n        \n    def step(self, action):\n        # Apply action to robot\n        self.robot.apply_action(action)\n        \n        # Observe new state\n        next_state = self.robot.get_state()\n        \n        # Calculate reward\n        reward = self.compute_reward(next_state)\n        \n        # Check termination\n        done = self.check_termination(next_state)\n        \n        return next_state, reward, done, {}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"isaac-gym-preview--isaac-lab",children:"Isaac Gym (Preview) / Isaac Lab"}),"\n",(0,s.jsx)(n.p,{children:"Massively parallel simulation on GPU."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Physics on GPU"}),": No CPU-GPU bottleneck."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel Envs"}),": Simulate thousands of robots simultaneously."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Isaac Gym Setup\nfrom isaacgym import gymapi\n\n# Create thousands of environments\nnum_envs = 4096\nenvs = []\n\nfor i in range(num_envs):\n    # Create env instance (very fast on GPU)\n    env = gym.create_env(sim, env_lower, env_upper, num_per_row)\n    \n    # Add robot actor\n    actor = gym.create_actor(env, robot_asset, pose, "robot", i, 1)\n    envs.append(env)\n\n# Parallel simulation step\ngym.simulate(sim)\ngym.fetch_results(sim, True)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"reward-engineering",children:"Reward Engineering"}),"\n",(0,s.jsx)(n.h3,{id:"the-art-of-shaping-rewards",children:"The Art of Shaping Rewards"}),"\n",(0,s.jsx)(n.p,{children:"Crafting the reward function is the most critical part of RL."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Sparse Reward"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"+1 for success, 0 otherwise."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Agent learns nothing if it never succeeds by chance."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Dense Reward"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Shaped reward guiding the agent."}),"\n",(0,s.jsxs)(n.li,{children:["Example for reaching task: ",(0,s.jsx)(n.code,{children:"$r = -d_{target} - \\lambda ||a||^2$"})," (Minimize distance and energy)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Curriculum Learning"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Start with easy tasks, gradually increase difficulty."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def compute_reward(self, robot_pos, target_pos, actions):\n    # Distance penalty\n    dist = torch.norm(robot_pos - target_pos, p=2, dim=-1)\n    reward_dist = 1.0 / (1.0 + dist**2)\n    \n    # Action penalty (discourage jerky motion)\n    reward_action = -torch.sum(actions**2, dim=-1)\n    \n    # Success bonus\n    reward_success = (dist < 0.05).float() * 10.0\n    \n    total_reward = reward_dist + 0.01 * reward_action + reward_success\n    return total_reward\n"})}),"\n",(0,s.jsx)(n.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(n.h3,{id:"the-reality-gap",children:"The Reality Gap"}),"\n",(0,s.jsx)(n.p,{children:"Simulation is perfect; reality is messy. Models trained in sim often fail in real life."}),"\n",(0,s.jsx)(n.h3,{id:"domain-randomization-dr",children:"Domain Randomization (DR)"}),"\n",(0,s.jsx)(n.p,{children:"Randomize physics params to make the policy robust."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamics Randomization"}),": Mass, friction, damping, motor strength."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual Randomization"}),": Lighting, textures, camera position."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Domain Randomization Config\nrandomization_params = {\n    'actor_params': {\n        'rigid_body_properties': {\n             'mass': {'range': [0.8, 1.2], 'operation': 'scaling'},\n             'friction': {'range': [0.5, 1.5], 'operation': 'scaling'}\n        },\n        'dof_properties': {\n             'stiffness': {'range': [0.8, 1.2], 'operation': 'scaling'},\n             'damping': {'range': [0.8, 1.2], 'operation': 'scaling'}\n        }\n    }\n}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"-weekly-project-rl-based-locomotion",children:"\ud83c\udfaf Weekly Project: RL-Based Locomotion"}),"\n",(0,s.jsx)(n.p,{children:"Train a quadruped robot or simple walker to move efficiently:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment"}),": Setup Isaac Gym with a terrain environment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Move forward as fast as possible without falling."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Algorithm"}),": Train using PPO with massive parallelism."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Shaping"}),": Experiment with different reward functions for smooth gait."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transfer"}),": Validate robustness by changing ground friction."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'This project creates a "brain" that learns the physics of walking from scratch!'}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reinforcement Learning"})," enables robots to learn complex skills from experience."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PPO"})," is the robust workhorse algorithm for robotic control."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Massively parallel simulation"})," (Isaac Gym) allows training in minutes, not days."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward engineering"}),' is crucial; "you get what you reward".']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Domain randomization"})," bridge the sim-to-real gap by making policies adaptable."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Congratulations!"})," You have completed the core technical modules of the Physical AI course. You now possess the knowledge to build, simulate, perceive, and control intelligent robots."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var r=i(6540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);