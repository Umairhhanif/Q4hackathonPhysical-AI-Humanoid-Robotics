"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[388],{1728:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-03-isaac-brain/week-09-perception","title":"Week 9: Perception","description":"Giving Robots the Sense of Sight and Touch","source":"@site/docs/module-03-isaac-brain/week-09-perception.md","sourceDirName":"module-03-isaac-brain","slug":"/module-03-isaac-brain/week-09-perception","permalink":"/docs/module-03-isaac-brain/week-09-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-03-isaac-brain/week-09-perception.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Week 9: Perception","sidebar_position":2},"sidebar":"modulesSidebar","previous":{"title":"Week 8: NVIDIA Isaac","permalink":"/docs/module-03-isaac-brain/week-08-isaac"},"next":{"title":"Perception & Control","permalink":"/docs/module-03-isaac-brain/perception"}}');var t=i(4848),s=i(8453);const r={title:"Week 9: Perception",sidebar_position:2},a="Week 9: AI-Powered Perception and Manipulation",l={},c=[{value:"Giving Robots the Sense of Sight and Touch",id:"giving-robots-the-sense-of-sight-and-touch",level:2},{value:"Perception Architectures",id:"perception-architectures",level:2},{value:"The Perception Pipeline",id:"the-perception-pipeline",level:3},{value:"Deep Learning for Robotics",id:"deep-learning-for-robotics",level:3},{value:"Object Detection and Segmentation",id:"object-detection-and-segmentation",level:2},{value:"YOLO (You Only Look Once)",id:"yolo-you-only-look-once",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"6D Pose Estimation",id:"6d-pose-estimation",level:2},{value:"Understanding 6D Pose",id:"understanding-6d-pose",level:3},{value:"Pose Estimation Techniques",id:"pose-estimation-techniques",level:3},{value:"Manipulation and Grasping",id:"manipulation-and-grasping",level:2},{value:"Grasp Detection",id:"grasp-detection",level:3},{value:"Motion Planning for Manipulation",id:"motion-planning-for-manipulation",level:3},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Image-Based Visual Servoing (IBVS)",id:"image-based-visual-servoing-ibvs",level:3},{value:"Position-Based Visual Servoing (PBVS)",id:"position-based-visual-servoing-pbvs",level:3},{value:"Imitation Learning",id:"imitation-learning",level:2},{value:"Learning from Demonstration (LfD)",id:"learning-from-demonstration-lfd",level:3},{value:"\ud83c\udfaf Weekly Project: Intelligent Pick-and-Place",id:"-weekly-project-intelligent-pick-and-place",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-9-ai-powered-perception-and-manipulation",children:"Week 9: AI-Powered Perception and Manipulation"})}),"\n",(0,t.jsx)(n.h2,{id:"giving-robots-the-sense-of-sight-and-touch",children:"Giving Robots the Sense of Sight and Touch"}),"\n",(0,t.jsxs)(n.p,{children:["This week transforms your robot from a blind machine into a perceptive agent. We'll explore ",(0,t.jsx)(n.strong,{children:"AI-powered perception"}),", enabling robots to identify, locate, and manipulate objects in 3D space. You'll move beyond simple camera feeds to deep learning models that understand the world, creating the bridge between raw sensor data and intelligent action."]}),"\n",(0,t.jsx)(n.h2,{id:"perception-architectures",children:"Perception Architectures"}),"\n",(0,t.jsx)(n.h3,{id:"the-perception-pipeline",children:"The Perception Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"A typical robotic perception pipeline consists of:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Acquisition"}),": Capturing data from cameras, LIDAR, and depth sensors."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Preprocessing"}),": Noise reduction, filtering, and normalization."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Extraction"}),": Identifying key points, edges, and textures."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inference"}),": Running AI models for detection, segmentation, and pose estimation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Post-processing"}),": Tracking, data association, and state estimation."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-for-robotics",children:"Deep Learning for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Robotics relies on specific neural network architectures:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CNNs (Convolutional Neural Networks)"}),": For image classification and object detection."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PointNet/PointNet++"}),": For 3D point cloud classification and segmentation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transfomers"}),": Vision transformers for sequence and attention-based tasks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Graph Neural Networks"}),": For relationship modeling and scene graphs."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"object-detection-and-segmentation",children:"Object Detection and Segmentation"}),"\n",(0,t.jsx)(n.h3,{id:"yolo-you-only-look-once",children:"YOLO (You Only Look Once)"}),"\n",(0,t.jsx)(n.p,{children:"Real-time object detection essential for fast robot control."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# YOLO inference with PyTorch\nimport torch\n\n# Load model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n\n# Inference\nresults = model('image.jpg')\n\n# Process results\npredictions = results.xyxy[0].cpu().numpy()\nfor pred in predictions:\n    x1, y1, x2, y2, conf, cls = pred\n    print(f\"Detected class {cls} with confidence {conf} at [{x1}, {y1}, {x2}, {y2}]\")\n"})}),"\n",(0,t.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Classifying every pixel in an image for navigable space detection."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Semantic segmentation with torchvision\nimport torchvision.transforms as T\nfrom torchvision.models.segmentation import fcn_resnet50\n\n# Load model\nmodel = fcn_resnet50(pretrained=True).eval()\n\n# Transform input\ntransform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\ninput_tensor = transform(image).unsqueeze(0)\n\n# Inference\noutput = model(input_tensor)['out'][0]\nprediction = output.argmax(0)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"6d-pose-estimation",children:"6D Pose Estimation"}),"\n",(0,t.jsx)(n.h3,{id:"understanding-6d-pose",children:"Understanding 6D Pose"}),"\n",(0,t.jsxs)(n.p,{children:["To manipulate an object, a robot must know its ",(0,t.jsx)(n.strong,{children:"6D Pose"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Position"}),": (x, y, z)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"3D Orientation"}),": (roll, pitch, yaw) or quaternion"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"pose-estimation-techniques",children:"Pose Estimation Techniques"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Correspondence-Based"}),": Matching 2D image features to 3D model points (PnP)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Template-Based"}),": Matching input against a library of rendered templates."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Direct Regression"}),": Neural networks predicting pose directly from images."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Pose estimation with DenseFusion (pseudo-code)\nclass PoseEstimator:\n    def estimate_pose(self, rgb, depth, mask, cam_K):\n        # Extract features\n        img_feat = self.cnn(rgb)\n        cloud_feat = self.pointnet(depth, mask, cam_K)\n        \n        # Fuse features\n        fused = self.fusion_layer(img_feat, cloud_feat)\n        \n        # Regress pose\n        translation = self.trans_head(fused)\n        rotation = self.rot_head(fused)\n        confidence = self.conf_head(fused)\n        \n        return self.refine_pose(translation, rotation, confidence)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"manipulation-and-grasping",children:"Manipulation and Grasping"}),"\n",(0,t.jsx)(n.h3,{id:"grasp-detection",children:"Grasp Detection"}),"\n",(0,t.jsx)(n.p,{children:"Identifying viable grasp points on an object."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"GraspNet Architecture"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Input: Point cloud or RGB-D image."}),"\n",(0,t.jsx)(n.li,{children:"Output: Set of 6D grasp poses with quality scores."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Grasp quality evaluation\ndef evaluate_grasp(gripper_pose, object_cloud):\n    # Check collisions with gripper\n    if check_collision(gripper_pose, object_cloud):\n        return 0.0\n        \n    # Check contact points\n    contacts = find_contacts(gripper_pose, object_cloud)\n    if len(contacts) < 2:\n        return 0.0\n        \n    # Calculate grasp metric (e.g., force closure)\n    return calculate_quality(contacts)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"motion-planning-for-manipulation",children:"Motion Planning for Manipulation"}),"\n",(0,t.jsx)(n.p,{children:"Moving the arm to the grasp pose without collisions."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# MoveIt! 2 planning in Python\nfrom moveit_configs_utils import MoveItConfigsBuilder\nfrom moveit.planning import MoveGroupInterface\n\n# Setup\nmove_group = MoveGroupInterface("manipulator", "base_link")\n\n# Define target\ntarget_pose = geometry_msgs.msg.Pose()\ntarget_pose.position.x = 0.5\ntarget_pose.position.y = 0.0\ntarget_pose.position.z = 0.5\ntarget_pose.orientation.w = 1.0\n\n# Plan and execute\nmove_group.set_pose_target(target_pose)\nplan = move_group.plan()\n\nif plan.success:\n    move_group.execute(plan)\nelse:\n    print("Planning failed!")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,t.jsx)(n.h3,{id:"image-based-visual-servoing-ibvs",children:"Image-Based Visual Servoing (IBVS)"}),"\n",(0,t.jsx)(n.p,{children:"Control robot motion to minimize error in the image plane."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Simple proportional controller for visual servoing\ndef update_control(target_pixels, current_pixels, Kp=0.01):\n    error = target_pixels - current_pixels\n    velocity_cmd = Kp * error\n    return velocity_cmd\n"})}),"\n",(0,t.jsx)(n.h3,{id:"position-based-visual-servoing-pbvs",children:"Position-Based Visual Servoing (PBVS)"}),"\n",(0,t.jsx)(n.p,{children:"Control robot motion based on estimated 3D pose error."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# PBVS control loop\ndef pose_servoing_loop(target_pose):\n    while not reached_target():\n        # Estimate current pose\n        current_pose = pose_estimator.get_pose()\n        \n        # Calculate error in 3D\n        error = compute_pose_error(target_pose, current_pose)\n        \n        # Compute velocity command (Cartesian velocity)\n        v_cartesian = Kp * error\n        \n        # Convert to joint velocities using Jacobian pseudoinverse\n        J = robot.get_jacobian()\n        v_joints = np.linalg.pinv(J) @ v_cartesian\n        \n        # Send to robot\n        robot.set_joint_velocities(v_joints)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,t.jsx)(n.h3,{id:"learning-from-demonstration-lfd",children:"Learning from Demonstration (LfD)"}),"\n",(0,t.jsx)(n.p,{children:"Teaching robots by showing them what to do."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kinesthetic Teaching"}),": Physically guiding the robot arm."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Teleoperation"}),": Controlling the robot remotely."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Video Demonstration"}),": Learning from human videos."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Behavioral Cloning (BC) Training Loop\ndef train_bc_policy(demos, policy_network):\n    optimizer = torch.optim.Adam(policy_network.parameters())\n    \n    for epoch in range(num_epochs):\n        for state, action in demos:\n            # Predict action\n            pred_action = policy_network(state)\n            \n            # Calculate loss (MSE)\n            loss = torch.nn.functional.mse_loss(pred_action, action)\n            \n            # Backpropagate\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-weekly-project-intelligent-pick-and-place",children:"\ud83c\udfaf Weekly Project: Intelligent Pick-and-Place"}),"\n",(0,t.jsx)(n.p,{children:"Build a complete pick-and-place system:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Use a camera to detect objects on a table."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pose Estimation"}),": Determine position and orientation of target object."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasp Planning"}),": Select optimal grasp points."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion Planning"}),": Plan collision-free path to grasp object."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution"}),": Pick up object and place it in a target container."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This project integrates perception, planning, and control into a cohesive robotic skill."}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception pipelines"})," convert raw sensor data into meaningful world representations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object detection and segmentation"})," identify what acts are in the scene."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"6D pose estimation"})," tells the robot exactly where objects are for interaction."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Motion planning"})," moves the robot safely to the target."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual servoing"})," provides closed-loop control for precise positioning."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Next week, we'll explore the pinnacle of robotic learning: Reinforcement Learning, where robots learn complex behaviors through trial and error."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);