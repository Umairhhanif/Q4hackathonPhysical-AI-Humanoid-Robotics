"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[424],{2397:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-01-ros2/week-02-nodes","title":"Week 2 - ROS 2 Nodes","description":"Understanding Intelligence Through Physical Embodiment","source":"@site/docs/module-01-ros2/week-02-nodes.md","sourceDirName":"module-01-ros2","slug":"/module-01-ros2/week-02-nodes","permalink":"/docs/module-01-ros2/week-02-nodes","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-01-ros2/week-02-nodes.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Week 2 - ROS 2 Nodes","sidebar_position":2},"sidebar":"modulesSidebar","previous":{"title":"Week 1: Introduction","permalink":"/docs/module-01-ros2/week-01-intro"},"next":{"title":"Week 3 - Topics & Messages","permalink":"/docs/module-01-ros2/week-03-topics"}}');var t=i(4848),o=i(8453);const r={title:"Week 2 - ROS 2 Nodes",sidebar_position:2},l="Week 2: Foundations of Physical AI",a={},c=[{value:"Understanding Intelligence Through Physical Embodiment",id:"understanding-intelligence-through-physical-embodiment",level:2},{value:"The Essence of Embodied Intelligence",id:"the-essence-of-embodied-intelligence",level:2},{value:"Beyond Digital Abstraction",id:"beyond-digital-abstraction",level:3},{value:"The Perception-Action Loop",id:"the-perception-action-loop",level:3},{value:"The Humanoid Robotics Landscape",id:"the-humanoid-robotics-landscape",level:2},{value:"Why Humanoids Matter",id:"why-humanoids-matter",level:3},{value:"Current State of the Art",id:"current-state-of-the-art",level:3},{value:"Sensor Systems: The Robot&#39;s Senses",id:"sensor-systems-the-robots-senses",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"Other Essential Sensors",id:"other-essential-sensors",level:3},{value:"From Digital to Physical: Understanding Physical Laws",id:"from-digital-to-physical-understanding-physical-laws",level:2},{value:"Newtonian Physics in Robotics",id:"newtonian-physics-in-robotics",level:3},{value:"The Challenge of Physical Uncertainty",id:"the-challenge-of-physical-uncertainty",level:3},{value:"Building Robust Physical AI Systems",id:"building-robust-physical-ai-systems",level:2},{value:"Probabilistic Reasoning",id:"probabilistic-reasoning",level:3},{value:"Safety and Reliability",id:"safety-and-reliability",level:3},{value:"\ud83c\udfc6 Weekly Project: Multi-Sensor Integration",id:"-weekly-project-multi-sensor-integration",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-2-foundations-of-physical-ai",children:"Week 2: Foundations of Physical AI"})}),"\n",(0,t.jsx)(n.h2,{id:"understanding-intelligence-through-physical-embodiment",children:"Understanding Intelligence Through Physical Embodiment"}),"\n",(0,t.jsxs)(n.p,{children:["This week, we dive deeper into the principles that distinguish Physical AI from traditional approaches. You'll explore how intelligence emerges from the interaction between ",(0,t.jsx)(n.strong,{children:"perception"}),", ",(0,t.jsx)(n.strong,{children:"cognition"}),", and ",(0,t.jsx)(n.strong,{children:"action"}),", and examine the current state of humanoid robotics that makes these concepts concrete."]}),"\n",(0,t.jsx)(n.h2,{id:"the-essence-of-embodied-intelligence",children:"The Essence of Embodied Intelligence"}),"\n",(0,t.jsx)(n.h3,{id:"beyond-digital-abstraction",children:"Beyond Digital Abstraction"}),"\n",(0,t.jsx)(n.p,{children:"Traditional AI treats intelligence as information processing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Input \u2192 Processing \u2192 Output"}),"\n",(0,t.jsx)(n.li,{children:"Clean, structured data"}),"\n",(0,t.jsx)(n.li,{children:"Deterministic environments"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Physical AI"})," recognizes that intelligence is fundamentally embodied:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),": Making sense of noisy, multi-modal sensory input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": Affecting the world through physical movement"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognition"}),": Reasoning that accounts for physical constraints"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning"}),": Adaptation through real-world interaction"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"the-perception-action-loop",children:"The Perception-Action Loop"}),"\n",(0,t.jsx)(n.p,{children:"At the heart of embodied intelligence is the continuous cycle:"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Sensory Input \u2192 Interpretation \u2192 Decision \u2192 Action \u2192 New Sensory Input"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This loop creates a dynamic relationship between agent and environment, where intelligence emerges from interaction rather than isolated computation."}),"\n",(0,t.jsx)(n.h2,{id:"the-humanoid-robotics-landscape",children:"The Humanoid Robotics Landscape"}),"\n",(0,t.jsx)(n.h3,{id:"why-humanoids-matter",children:"Why Humanoids Matter"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots represent the ultimate challenge in Physical AI:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human-Centered Environments"}),": Designed to operate in spaces built for humans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Interaction"}),": Capable of communicating through gesture and movement"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"General-Purpose Manipulation"}),": Hands and arms that can use human tools"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Social Intelligence"}),": Understanding and responding to human behavior"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"current-state-of-the-art",children:"Current State of the Art"}),"\n",(0,t.jsx)(n.admonition,{title:"Industrial Humanoids",type:"info",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Boston Dynamics Atlas"}),": Advanced bipedal locomotion, dynamic balancing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tesla Optimus"}),": Mass-produced general-purpose humanoid"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Figure AI"}),": Specialized for warehouse and manufacturing tasks"]}),"\n"]})}),"\n",(0,t.jsx)(n.admonition,{title:"Research Platforms",type:"tip",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA Isaac Humanoid"}),": AI-powered perception and control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Agility Robotics Digit"}),": Humanoid for industrial applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unitree G1"}),": Affordable platform for education and research"]}),"\n"]})}),"\n",(0,t.jsx)(n.admonition,{title:"Social and Service Robots",type:"note",children:(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SoftBank Pepper"}),": Emotional interaction and service tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Honda Asimo"}),": Pioneering work in bipedal locomotion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"UBTECH Walker"}),": Educational and entertainment applications"]}),"\n"]})}),"\n",(0,t.jsx)(n.h2,{id:"sensor-systems-the-robots-senses",children:"Sensor Systems: The Robot's Senses"}),"\n",(0,t.jsx)(n.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,t.jsx)(n.p,{children:"Cameras provide the richest sensory input but require sophisticated processing."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RGB Cameras"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Color Information: Object recognition and scene understanding"}),"\n",(0,t.jsx)(n.li,{children:"Motion Detection: Tracking moving objects and self-motion"}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Limit"}),": Sensitive to lighting conditions and occlusions"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Sensors"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Structured Light: Project patterns to calculate distance"}),"\n",(0,t.jsx)(n.li,{children:"Time-of-Flight: Measure time for light to return"}),"\n",(0,t.jsx)(n.li,{children:"Stereo Vision: Use two cameras for depth calculation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Basic depth processing example"',children:"import numpy as np\n\nclass DepthProcessor:\n    def __init__(self):\n        self.max_range = 10.0  # meters\n        \n    def process_depth_image(self, depth_image):\n        # Convert to point cloud\n        height, width = depth_image.shape\n        points = []\n        \n        for v in range(height):\n            for u in range(width):\n                depth = depth_image[v, u]\n                if depth > 0 and depth < self.max_range:\n                    # Convert to 3D coordinates\n                    x = (u - width/2) * depth / self.focal_length\n                    y = (v - height/2) * depth / self.focal_length\n                    z = depth\n                    points.append([x, y, z])\n        \n        return np.array(points)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"other-essential-sensors",children:"Other Essential Sensors"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LIDAR Systems"}),": Light Detection and Ranging (Rotating or Solid-State) for SLAM and mapping."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"IMUs"}),": Accelerometers + Gyroscopes for orientation and balance."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Force/Tactile"}),": Measuring interaction forces and touch pressure."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"from-digital-to-physical-understanding-physical-laws",children:"From Digital to Physical: Understanding Physical Laws"}),"\n",(0,t.jsx)(n.h3,{id:"newtonian-physics-in-robotics",children:"Newtonian Physics in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Robots must internalize fundamental physical principles."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Simple rigid body dynamics"',children:"class RigidBody:\n    def __init__(self, mass, inertia):\n        self.mass = mass\n        self.inertia = inertia\n        self.position = np.zeros(3)\n        self.velocity = np.zeros(3)\n    \n    def apply_force(self, force, torque, dt):\n        # Linear motion\n        acceleration = force / self.mass\n        self.velocity += acceleration * dt\n        self.position += self.velocity * dt\n"})}),"\n",(0,t.jsx)(n.h3,{id:"the-challenge-of-physical-uncertainty",children:"The Challenge of Physical Uncertainty"}),"\n",(0,t.jsx)(n.p,{children:"All physical sensors introduce uncertainty:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Measurement Noise"}),": Random variations."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Bias"}),": Systematic errors."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environmental Variability"}),": Lighting, Friction, Weather."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"building-robust-physical-ai-systems",children:"Building Robust Physical AI Systems"}),"\n",(0,t.jsx)(n.h3,{id:"probabilistic-reasoning",children:"Probabilistic Reasoning"}),"\n",(0,t.jsx)(n.p,{children:"Using probability to handle uncertainty (e.g., Kalman Filters)."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="Kalman filter example"',children:"def update(self, measurement):\n    # Measurement update\n    H = np.zeros((3, 6))\n    H[0:3, 0:3] = np.eye(3)  # position measurement\n    \n    innovation = measurement - H @ self.state\n    # ... calculation of Kalman gain ...\n    self.state = self.state + kalman_gain @ innovation\n"})}),"\n",(0,t.jsx)(n.h3,{id:"safety-and-reliability",children:"Safety and Reliability"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fail-Safe Behaviors"}),": Default actions when systems fail."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Graceful Degradation"}),": Maintaining partial functionality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human Oversight"}),": Mechanisms for human intervention."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"-weekly-project-multi-sensor-integration",children:"\ud83c\udfc6 Weekly Project: Multi-Sensor Integration"}),"\n",(0,t.jsx)(n.p,{children:"Create a simulated robot that integrates multiple sensor modalities:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision Processing"}),": Object detection in camera images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Sensing"}),": Distance measurement and 3D reconstruction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inertial Sensing"}),": Orientation and motion tracking"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining multiple inputs for robust state estimation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Embodied intelligence"})," requires understanding physical laws."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor systems"})," are inherently noisy and uncertain."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Humanoid robotics"})," represents the ultimate Physical AI challenge."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety and reliability"})," are non-negotiable."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Next week, we'll begin exploring ",(0,t.jsx)(n.strong,{children:"ROS 2"}),", the communication framework that enables these concepts to work together in practice."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);