"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[224],{4599:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-04-vla/week-13-robotics","title":"Week 13: Conversational Robotics","description":"Integrating GPT Models for Natural Human-Robot Interaction","source":"@site/docs/module-04-vla/week-13-robotics.md","sourceDirName":"module-04-vla","slug":"/module-04-vla/week-13-robotics","permalink":"/docs/module-04-vla/week-13-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-04-vla/week-13-robotics.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Week 13: Conversational Robotics","sidebar_position":3},"sidebar":"modulesSidebar","previous":{"title":"Week 12: Kinematics & Dynamics","permalink":"/docs/module-04-vla/week-12-kinematics"},"next":{"title":"References","permalink":"/docs/references"}}');var o=t(4848),s=t(8453);const i={title:"Week 13: Conversational Robotics",sidebar_position:3},r="Week 13: Conversational Robotics - Making Robots Truly Conversational",l={},c=[{value:"Integrating GPT Models for Natural Human-Robot Interaction",id:"integrating-gpt-models-for-natural-human-robot-interaction",level:2},{value:"Speech Recognition and Natural Language Understanding",id:"speech-recognition-and-natural-language-understanding",level:2},{value:"Automatic Speech Recognition (ASR)",id:"automatic-speech-recognition-asr",level:3},{value:"Large Language Model Integration",id:"large-language-model-integration",level:2},{value:"GPT-Powered Task Understanding",id:"gpt-powered-task-understanding",level:3},{value:"Vision-Language Grounding",id:"vision-language-grounding",level:2},{value:"Connecting Language to Visual Perception",id:"connecting-language-to-visual-perception",level:3},{value:"Multi-Modal Dialogue Management",id:"multi-modal-dialogue-management",level:2},{value:"Conversational State Tracking",id:"conversational-state-tracking",level:3},{value:"Complete Conversational System Integration",id:"complete-conversational-system-integration",level:2},{value:"Putting It All Together",id:"putting-it-all-together",level:3},{value:"\ud83c\udfaf Weekly Project: Conversational Humanoid System",id:"-weekly-project-conversational-humanoid-system",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"week-13-conversational-robotics---making-robots-truly-conversational",children:"Week 13: Conversational Robotics - Making Robots Truly Conversational"})}),"\n",(0,o.jsx)(n.h2,{id:"integrating-gpt-models-for-natural-human-robot-interaction",children:"Integrating GPT Models for Natural Human-Robot Interaction"}),"\n",(0,o.jsx)(n.p,{children:"This final week brings everything together by teaching you how to create truly conversational robots. You'll learn to integrate large language models like GPT with robotic perception and control systems, enabling natural language understanding, contextual dialogue, and voice-driven task execution. By week's end, you'll have built a conversational humanoid capable of understanding complex commands and engaging in meaningful interaction."}),"\n",(0,o.jsx)(n.h2,{id:"speech-recognition-and-natural-language-understanding",children:"Speech Recognition and Natural Language Understanding"}),"\n",(0,o.jsx)(n.h3,{id:"automatic-speech-recognition-asr",children:"Automatic Speech Recognition (ASR)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\nfrom transformers import pipeline\nimport torch\n\nclass SpeechProcessor:\n    \"\"\"Complete speech processing pipeline\"\"\"\n    \n    def __init__(self):\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        \n        # Calibrate for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\n        \n        # Language model for intent classification\n        self.intent_classifier = pipeline(\n            \"text-classification\",\n            model=\"facebook/bart-large-mnli\",\n            device=0 if torch.cuda.is_available() else -1\n        )\n        \n        # Named entity recognition\n        self.ner_pipeline = pipeline(\n            \"ner\",\n            model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n            aggregation_strategy=\"simple\"\n        )\n    \n    def listen_and_transcribe(self, timeout=5):\n        \"\"\"Listen to speech and convert to text\"\"\"\n        \n        try:\n            with self.microphone as source:\n                self.get_logger().info(\"Listening...\")\n                audio = self.recognizer.listen(source, timeout=timeout)\n                \n            # Use Google's speech recognition\n            text = self.recognizer.recognize_google(audio)\n            self.get_logger().info(f\"Transcribed: {text}\")\n            \n            return text\n            \n        except sr.WaitTimeoutError:\n            return None\n        except sr.UnknownValueError:\n            self.get_logger().warning(\"Could not understand audio\")\n            return None\n        except sr.RequestError as e:\n            self.get_logger().error(f\"Speech recognition error: {e}\")\n            return None\n    \n    def analyze_utterance(self, text):\n        \"\"\"Analyze transcribed text for intent and entities\"\"\"\n        \n        # Classify intent\n        intent_result = self.intent_classifier(\n            text,\n            candidate_labels=[\n                \"command\", \"question\", \"statement\", \n                \"clarification_request\", \"confirmation\"\n            ]\n        )\n        intent = intent_result['labels'][0]\n        intent_confidence = intent_result['scores'][0]\n        \n        # Extract entities\n        entities = self.ner_pipeline(text)\n        \n        # Parse objects, locations, actions\n        parsed_entities = self.parse_entities(entities)\n        \n        return {\n            'intent': intent,\n            'confidence': intent_confidence,\n            'entities': parsed_entities,\n            'raw_text': text\n        }\n    \n    def parse_entities(self, ner_results):\n        \"\"\"Parse NER results into structured entities\"\"\"\n        \n        entities = {\n            'objects': [],\n            'locations': [],\n            'actions': [],\n            'quantities': []\n        }\n        \n        for entity in ner_results:\n            entity_text = entity['word']\n            entity_type = entity['entity_group']\n            \n            if entity_type in ['MISC', 'ORG']:  # Often objects\n                entities['objects'].append(entity_text.lower())\n            elif entity_type == 'LOC':\n                entities['locations'].append(entity_text.lower())\n            elif entity_type == 'PER':  # Could be actions\n                entities['actions'].append(entity_text.lower())\n        \n        # Additional parsing for common patterns\n        # \"pick up the red box\" -> object: \"red box\", action: \"pick up\"\n        text_lower = ner_results[0]['word'].lower() if ner_results else \"\"\n        \n        if 'pick up' in text_lower or 'grab' in text_lower:\n            entities['actions'].append('pick_up')\n        if 'put' in text_lower or 'place' in text_lower:\n            entities['actions'].append('place')\n        if 'go to' in text_lower or 'move to' in text_lower:\n            entities['actions'].append('navigate')\n        \n        return entities\n"})}),"\n",(0,o.jsx)(n.h2,{id:"large-language-model-integration",children:"Large Language Model Integration"}),"\n",(0,o.jsx)(n.h3,{id:"gpt-powered-task-understanding",children:"GPT-Powered Task Understanding"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nfrom typing import Dict, List, Optional\nimport json\n\nclass GPTTaskPlanner:\n    """GPT-powered task planning and reasoning"""\n    \n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        openai.api_key = api_key\n        self.model = model\n        \n        # System prompt for robotic task planning\n        self.system_prompt = """\n        You are an intelligent task planning assistant for a humanoid robot.\n        Your role is to understand natural language commands and break them down\n        into executable robotic actions.\n        \n        Available actions:\n        - navigate_to(location): Move to a specified location\n        - pick_up(object): Grasp an object\n        - place_at(location): Place held object at location\n        - look_at(object/location): Orient gaze toward target\n        - speak(message): Generate speech output\n        - wait(seconds): Pause execution\n        \n        Locations: table, shelf, counter, floor, chair\n        Objects: red_box, blue_cup, green_book, yellow_ball, white_plate\n        \n        Respond with a JSON object containing:\n        - "actions": List of action dictionaries with "type" and "parameters"\n        - "clarifications": List of questions if command is ambiguous\n        - "confidence": Float between 0-1 indicating certainty\n        """\n    \n    def plan_task(self, command: str, context: Dict = None) -> Dict:\n        """Convert natural language command to robotic task plan"""\n        \n        # Build context-aware prompt\n        prompt = f"Command: {command}\\n"\n        if context:\n            prompt += f"Context: {json.dumps(context)}\\n"\n        prompt += "\\nGenerate task plan:"\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,  # Low temperature for consistent planning\n                max_tokens=500\n            )\n            \n            # Parse JSON response\n            result_text = response.choices[0].message.content.strip()\n            \n            # Extract JSON from response\n            json_start = result_text.find(\'{\')\n            json_end = result_text.rfind(\'}\') + 1\n            json_str = result_text[json_start:json_end]\n            \n            plan = json.loads(json_str)\n            \n            # Validate plan structure\n            if not self.validate_plan(plan):\n                return self.create_fallback_plan(command)\n            \n            return plan\n            \n        except Exception as e:\n            self.logger.error(f"GPT planning failed: {e}")\n            return self.create_fallback_plan(command)\n    \n    def validate_plan(self, plan: Dict) -> bool:\n        """Validate generated task plan"""\n        \n        if \'actions\' not in plan:\n            return False\n        \n        if not isinstance(plan[\'actions\'], list):\n            return False\n        \n        # Check each action\n        for action in plan[\'actions\']:\n            if \'type\' not in action:\n                return False\n            \n            if action[\'type\'] not in [\'navigate_to\', \'pick_up\', \'place_at\', \n                                    \'look_at\', \'speak\', \'wait\']:\n                return False\n        \n        return True\n    \n    def create_fallback_plan(self, command: str) -> Dict:\n        """Create simple fallback plan when GPT fails"""\n        \n        return {\n            "actions": [\n                {\n                    "type": "speak",\n                    "parameters": {"message": "I\'m not sure how to help with that. Could you rephrase your request?"}\n                }\n            ],\n            "clarifications": ["Could you please rephrase your request more clearly?"],\n            "confidence": 0.0\n        }\n    \n    def refine_plan_with_context(self, plan: Dict, visual_context: Dict) -> Dict:\n        """Refine plan based on visual perception"""\n        \n        # Use GPT to incorporate visual information\n        context_prompt = f"""\n        Original plan: {json.dumps(plan)}\n        Visual context: {json.dumps(visual_context)}\n        \n        Refine the plan based on current visual observations.\n        Update object locations, add clarification requests if needed,\n        or modify actions based on what the robot can actually see.\n        """\n        \n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=[\n                    {"role": "system", "content": "You are a plan refinement assistant for robots."},\n                    {"role": "user", "content": context_prompt}\n                ],\n                temperature=0.2,\n                max_tokens=300\n            )\n            \n            refined_plan = json.loads(response.choices[0].message.content)\n            return refined_plan\n            \n        except Exception as e:\n            self.logger.error(f"Plan refinement failed: {e}")\n            return plan\n'})}),"\n",(0,o.jsx)(n.h2,{id:"vision-language-grounding",children:"Vision-Language Grounding"}),"\n",(0,o.jsx)(n.h3,{id:"connecting-language-to-visual-perception",children:"Connecting Language to Visual Perception"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from transformers import CLIPProcessor, CLIPModel\nimport torch\nfrom PIL import Image\n\nclass VisionLanguageGrounder:\n    """Ground language descriptions in visual scenes"""\n    \n    def __init__(self):\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        \n        # Load CLIP model\n        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)\n        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n        \n        # Object vocabulary\n        self.object_classes = [\n            "red box", "blue cup", "green book", "yellow ball", "white plate",\n            "table", "shelf", "counter", "floor", "chair"\n        ]\n    \n    def ground_description(self, description: str, image: Image.Image) -> Dict:\n        """Find objects in image that match description"""\n        \n        # Prepare text inputs\n        text_inputs = [f"a photo of a {obj}" for obj in self.object_classes]\n        text_inputs.append(f"a photo of {description}")\n        \n        # Prepare image input\n        image_input = self.processor(images=image, return_tensors="pt").to(self.device)\n        \n        # Encode text and image\n        text_inputs_processed = self.processor(\n            text=text_inputs, return_tensors="pt", padding=True\n        ).to(self.device)\n        \n        with torch.no_grad():\n            image_features = self.model.get_image_features(**image_input)\n            text_features = self.model.get_text_features(**text_inputs_processed)\n            \n            # Compute similarities\n            similarities = torch.cosine_similarity(\n                image_features.unsqueeze(1), \n                text_features.unsqueeze(0), \n                dim=2\n            )\n            \n            # Get best matches\n            best_matches = torch.topk(similarities[0], k=3)\n            \n            results = []\n            for idx, score in zip(best_matches.indices, best_matches.values):\n                obj_name = self.object_classes[idx] if idx < len(self.object_classes) else description\n                results.append({\n                    \'object\': obj_name,\n                    \'confidence\': score.item(),\n                    \'is_direct_match\': idx == len(self.object_classes)\n                })\n        \n        return {\n            \'matches\': results,\n            \'best_match\': results[0] if results else None\n        }\n    \n    def resolve_references(self, command: str, detected_objects: List[Dict]) -> Dict:\n        """Resolve linguistic references to detected objects"""\n        \n        # Extract referring expressions\n        references = self.extract_references(command)\n        \n        resolved = {}\n        \n        for ref in references:\n            # Find best matching detected object\n            best_match = None\n            best_score = 0\n            \n            for obj in detected_objects:\n                # Compute matching score based on description similarity\n                score = self.compute_reference_match(ref, obj)\n                \n                if score > best_score:\n                    best_score = score\n                    best_match = obj\n            \n            if best_match and best_score > 0.5:\n                resolved[ref] = best_match\n        \n        return resolved\n    \n    def extract_references(self, command: str) -> List[str]:\n        """Extract object references from command"""\n        \n        # Simple pattern matching (could be enhanced with NLP)\n        words = command.lower().split()\n        references = []\n        \n        # Look for adjective + noun patterns\n        colors = [\'red\', \'blue\', \'green\', \'yellow\', \'white\']\n        objects = [\'box\', \'cup\', \'book\', \'ball\', \'plate\']\n        \n        i = 0\n        while i < len(words) - 1:\n            if words[i] in colors and words[i+1] in objects:\n                references.append(f"{words[i]} {words[i+1]}")\n                i += 2\n            else:\n                i += 1\n        \n        return references\n    \n    def compute_reference_match(self, reference: str, detected_obj: Dict) -> float:\n        """Compute how well a reference matches a detected object"""\n        \n        ref_words = set(reference.lower().split())\n        obj_words = set(detected_obj[\'class\'].lower().split())\n        \n        # Jaccard similarity\n        intersection = len(ref_words & obj_words)\n        union = len(ref_words | obj_words)\n        \n        return intersection / union if union > 0 else 0\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-dialogue-management",children:"Multi-Modal Dialogue Management"}),"\n",(0,o.jsx)(n.h3,{id:"conversational-state-tracking",children:"Conversational State Tracking"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from enum import Enum\nfrom typing import List, Dict, Any\n\nclass DialogueState(Enum):\n    IDLE = \"idle\"\n    LISTENING = \"listening\"\n    PROCESSING = \"processing\"\n    CLARIFYING = \"clarifying\"\n    EXECUTING = \"executing\"\n    FEEDBACK = \"feedback\"\n\nclass DialogueManager:\n    \"\"\"Manage conversational state and context\"\"\"\n    \n    def __init__(self):\n        self.state = DialogueState.IDLE\n        self.conversation_history = []\n        self.current_task = None\n        self.pending_clarifications = []\n        \n        # Context tracking\n        self.mentioned_objects = set()\n        self.mentioned_locations = set()\n        self.user_preferences = {}\n    \n    def process_utterance(self, utterance: Dict, visual_context: Dict = None) -> Dict:\n        \"\"\"Process user utterance and generate response\"\"\"\n        \n        # Add to conversation history\n        self.conversation_history.append({\n            'speaker': 'user',\n            'content': utterance,\n            'timestamp': self.get_timestamp()\n        })\n        \n        # Update context\n        self.update_context(utterance)\n        \n        # State machine logic\n        if self.state == DialogueState.IDLE:\n            response = self.handle_new_command(utterance, visual_context)\n        elif self.state == DialogueState.CLARIFYING:\n            response = self.handle_clarification(utterance)\n        elif self.state == DialogueState.EXECUTING:\n            response = self.handle_execution_feedback(utterance)\n        else:\n            response = self.handle_general_input(utterance)\n        \n        # Add response to history\n        self.conversation_history.append({\n            'speaker': 'robot',\n            'content': response,\n            'timestamp': self.get_timestamp()\n        })\n        \n        return response\n    \n    def handle_new_command(self, utterance: Dict, visual_context: Dict) -> Dict:\n        \"\"\"Handle new command from user\"\"\"\n        \n        if utterance['intent'] == 'command':\n            # Generate task plan\n            task_plan = self.generate_task_plan(utterance, visual_context)\n            \n            if task_plan['confidence'] > 0.7:\n                self.state = DialogueState.EXECUTING\n                self.current_task = task_plan\n                \n                return {\n                    'type': 'confirmation',\n                    'message': f\"I understand you want me to {utterance['raw_text']}. Starting now.\",\n                    'task_plan': task_plan\n                }\n            else:\n                # Need clarification\n                self.state = DialogueState.CLARIFYING\n                self.pending_clarifications = task_plan.get('clarifications', [])\n                \n                return {\n                    'type': 'clarification_request',\n                    'message': \"I need some clarification to help you better.\",\n                    'questions': self.pending_clarifications\n                }\n        else:\n            return {\n                'type': 'acknowledgment',\n                'message': self.generate_acknowledgment(utterance)\n            }\n    \n    def handle_clarification(self, utterance: Dict) -> Dict:\n        \"\"\"Handle clarification response\"\"\"\n        \n        # Update context with clarification\n        self.update_context(utterance)\n        \n        # Re-plan with additional information\n        task_plan = self.generate_task_plan(\n            self.conversation_history[-2]['content'],  # Original command\n            additional_context=utterance\n        )\n        \n        if task_plan['confidence'] > 0.6:\n            self.state = DialogueState.EXECUTING\n            self.current_task = task_plan\n            self.pending_clarifications = []\n            \n            return {\n                'type': 'confirmation',\n                'message': \"Thanks for the clarification. I understand now.\",\n                'task_plan': task_plan\n            }\n        else:\n            return {\n                'type': 'further_clarification',\n                'message': \"I still need more information.\",\n                'questions': task_plan.get('clarifications', [])\n            }\n    \n    def handle_execution_feedback(self, utterance: Dict) -> Dict:\n        \"\"\"Handle feedback during task execution\"\"\"\n        \n        if 'success' in utterance.get('entities', {}):\n            self.state = DialogueState.IDLE\n            self.current_task = None\n            \n            return {\n                'type': 'completion_acknowledgment',\n                'message': \"Great! Task completed successfully.\"\n            }\n        elif 'problem' in utterance.get('entities', {}):\n            # Handle execution issues\n            return {\n                'type': 'error_handling',\n                'message': \"I encountered an issue. Let me try a different approach.\",\n                'action': 'replanning'\n            }\n        else:\n            return {\n                'type': 'status_update',\n                'message': f\"Task is {self.get_task_progress()}% complete.\"\n            }\n    \n    def update_context(self, utterance: Dict):\n        \"\"\"Update conversation context\"\"\"\n        \n        entities = utterance.get('entities', {})\n        \n        # Track mentioned objects and locations\n        self.mentioned_objects.update(entities.get('objects', []))\n        self.mentioned_locations.update(entities.get('locations', []))\n        \n        # Learn user preferences\n        if 'preference' in utterance:\n            self.user_preferences.update(utterance['preference'])\n    \n    def generate_task_plan(self, utterance: Dict, visual_context: Dict = None, \n                          additional_context: Dict = None) -> Dict:\n        \"\"\"Generate task plan from utterance\"\"\"\n        \n        # Use GPT planner with context\n        planner = GPTTaskPlanner()\n        \n        context = {\n            'mentioned_objects': list(self.mentioned_objects),\n            'mentioned_locations': list(self.mentioned_locations),\n            'visual_context': visual_context,\n            'additional_context': additional_context\n        }\n        \n        return planner.plan_task(utterance['raw_text'], context)\n    \n    def generate_acknowledgment(self, utterance: Dict) -> str:\n        \"\"\"Generate appropriate acknowledgment\"\"\"\n        \n        if utterance['intent'] == 'question':\n            return \"That's an interesting question. Let me think about that.\"\n        elif utterance['intent'] == 'statement':\n            return \"I understand. Is there something specific you'd like me to help with?\"\n        else:\n            return \"I'm here to help. What would you like me to do?\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"complete-conversational-system-integration",children:"Complete Conversational System Integration"}),"\n",(0,o.jsx)(n.h3,{id:"putting-it-all-together",children:"Putting It All Together"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ConversationalHumanoid:\n    \"\"\"Complete conversational humanoid robot system\"\"\"\n    \n    def __init__(self):\n        # Initialize components\n        self.speech_processor = SpeechProcessor()\n        self.task_planner = GPTTaskPlanner(api_key=\"your-openai-key\")\n        self.vision_grounder = VisionLanguageGrounder()\n        self.dialogue_manager = DialogueManager()\n        \n        # Robot control interfaces\n        self.navigation_controller = NavigationController()\n        self.manipulation_controller = ManipulationController()\n        self.speech_synthesizer = SpeechSynthesizer()\n        \n        # Perception\n        self.camera = Camera()\n        self.object_detector = ObjectDetector()\n    \n    def run_conversation_loop(self):\n        \"\"\"Main conversational loop\"\"\"\n        \n        self.speech_synthesizer.speak(\"Hello! I'm ready to help. What would you like me to do?\")\n        \n        while True:\n            try:\n                # Listen for command\n                text = self.speech_processor.listen_and_transcribe(timeout=10)\n                \n                if text is None:\n                    continue\n                \n                # Analyze utterance\n                utterance = self.speech_processor.analyze_utterance(text)\n                \n                # Get visual context\n                image = self.camera.capture_image()\n                detections = self.object_detector.detect(image)\n                visual_context = {\n                    'image': image,\n                    'detections': detections\n                }\n                \n                # Process through dialogue manager\n                response = self.dialogue_manager.process_utterance(utterance, visual_context)\n                \n                # Execute response\n                self.execute_response(response)\n                \n            except KeyboardInterrupt:\n                break\n            except Exception as e:\n                self.logger.error(f\"Conversation loop error: {e}\")\n                self.speech_synthesizer.speak(\"I'm sorry, I encountered an error. Let's try again.\")\n    \n    def execute_response(self, response: Dict):\n        \"\"\"Execute the generated response\"\"\"\n        \n        if response['type'] == 'confirmation':\n            # Start task execution\n            self.execute_task_plan(response['task_plan'])\n            \n        elif response['type'] == 'clarification_request':\n            # Ask for clarification\n            for question in response['questions']:\n                self.speech_synthesizer.speak(question)\n        \n        elif response['type'] == 'completion_acknowledgment':\n            # Acknowledge completion\n            self.speech_synthesizer.speak(response['message'])\n        \n        # Speak the message\n        if 'message' in response:\n            self.speech_synthesizer.speak(response['message'])\n    \n    def execute_task_plan(self, task_plan: Dict):\n        \"\"\"Execute a generated task plan\"\"\"\n        \n        for action in task_plan['actions']:\n            action_type = action['type']\n            params = action.get('parameters', {})\n            \n            try:\n                if action_type == 'navigate_to':\n                    self.navigation_controller.navigate_to(params['location'])\n                    \n                elif action_type == 'pick_up':\n                    # Ground object reference\n                    grounded_obj = self.vision_grounder.ground_description(\n                        params['object'], self.camera.capture_image()\n                    )\n                    if grounded_obj['best_match']:\n                        self.manipulation_controller.pick_up(grounded_obj['best_match'])\n                \n                elif action_type == 'place_at':\n                    self.manipulation_controller.place_at(params['location'])\n                \n                elif action_type == 'look_at':\n                    self.navigation_controller.look_at(params['target'])\n                \n                elif action_type == 'speak':\n                    self.speech_synthesizer.speak(params['message'])\n                \n                elif action_type == 'wait':\n                    time.sleep(params['seconds'])\n                \n                # Provide execution feedback\n                self.speech_synthesizer.speak(f\"Completed {action_type}\")\n                \n            except Exception as e:\n                self.logger.error(f\"Action execution failed: {e}\")\n                self.speech_synthesizer.speak(f\"I had trouble with {action_type}. Let me try something else.\")\n                break\n"})}),"\n",(0,o.jsx)(n.h2,{id:"-weekly-project-conversational-humanoid-system",children:"\ud83c\udfaf Weekly Project: Conversational Humanoid System"}),"\n",(0,o.jsx)(n.p,{children:"Build a complete conversational humanoid that can:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Listen and transcribe natural language commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language Understanding"}),": Parse intents and extract entities using GPT"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision-Language Grounding"}),": Connect linguistic references to visual detections"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Planning"}),": Generate executable action sequences from natural language"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dialogue Management"}),": Handle clarifications and maintain conversation context"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Modal Execution"}),": Coordinate speech, vision, and physical actions"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This capstone project demonstrates the complete integration of conversational AI with humanoid robotics."}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech recognition"})," enables natural voice interfaces for human-robot interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Large language models"})," provide sophisticated language understanding and task planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision-language grounding"})," connects words to the physical world through perception"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dialogue management"})," maintains context and handles complex interactions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-modal integration"})," creates truly conversational robots capable of natural interaction"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Mastering conversational robotics transforms robots from programmed machines into intelligent partners capable of understanding and collaborating with humans in natural, intuitive ways."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Congratulations!"})," You have completed the Physical AI curriculum. The conversational humanoid you've built represents the cutting edge of embodied intelligence\u2014a system that can perceive, understand, and act upon the world with human-like capabilities."]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);